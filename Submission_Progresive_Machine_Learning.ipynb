{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE91v2wCudoBdLBRnVPiUv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irfan-lie92/Machine_LearningCNN/blob/main/Submission_Progresive_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Proyek Klasifikasi Gambar: cat_dog_bird_dataset**\n",
        "\n",
        "\n",
        "- **Nama:** IRFAN ALI\n",
        "- **Email:** ir.vanaly@gmail.com\n",
        "- **ID Dicoding:** irfanlie92"
      ],
      "metadata": {
        "id": "iamq8kzqjqBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Semua Packages/Library yang Digunakan"
      ],
      "metadata": {
        "id": "NYvKEm2JlBaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle tensorflowjs\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pathlib\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflowjs as tfjs\n",
        "from google.colab import files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gyH2dmy3kUZR",
        "outputId": "931617e2-1bfd-4183-cc1d-b96c8dc8e81a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.11/dist-packages (4.22.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.10.6)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.5.3)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.5.3)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.11.20)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.76)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.9)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (1.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.14.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.13.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.1.3)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (24.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2025.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.23.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"margin:0px;\">ğŸŒ² Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n",
              "    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n",
              "        Decision Forests</a> using the same algorithms but with more features and faster\n",
              "    training!\n",
              "</p>\n",
              "<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            Old code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import tensorflow_decision_forests as tfdf\n",
              "\n",
              "tf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\n",
              "model = tfdf.keras.RandomForestModel(label=\"l\")\n",
              "model.fit(tf_ds)\n",
              "</pre>\n",
              "    </div>\n",
              "    <div style=\"width: 5px;\"></div>\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            New code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import ydf\n",
              "\n",
              "model = ydf.RandomForestLearner(label=\"l\").train(ds)\n",
              "</pre>\n",
              "    </div>\n",
              "</div>\n",
              "<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n",
              "        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n",
              "        guide</a>)</p>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "eGGeAZC_lK8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "YacYt_jDlOfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# 1. Upload kaggle.json\n",
        "# ===============================\n",
        "print(\"ğŸ“‚ Silakan upload file kaggle.json dari komputer Anda...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Pastikan folder ~/.kaggle ada\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Pindahkan kaggle.json ke folder config Kaggle\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, \"/root/.kaggle/kaggle.json\")\n",
        "\n",
        "# Ubah permission agar aman\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "# ===============================\n",
        "# 2. Install Kaggle CLI\n",
        "# ===============================\n",
        "!pip install kaggle --upgrade --quiet\n",
        "\n",
        "# ===============================\n",
        "# 3. Tes koneksi Kaggle\n",
        "# ===============================\n",
        "!kaggle datasets list -s \"books\"\n",
        "\n",
        "print(\"\\nâœ… kaggle.json berhasil diunggah dan terhubung.\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Download dataset\n",
        "# ===============================\n",
        "os.makedirs(\"/content/dataset\", exist_ok=True)\n",
        "!kaggle datasets download -d mahmoudnoor/high-resolution-catdogbird-image-dataset-13000 -p /content/dataset\n",
        "\n",
        "# ===============================\n",
        "# 5. Ekstrak dataset\n",
        "# ===============================\n",
        "zip_path = \"/content/dataset/high-resolution-catdogbird-image-dataset-13000.zip\"\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"/content/dataset\")\n",
        "    print(\"âœ… Dataset berhasil diekstrak di /content/dataset\")\n",
        "else:\n",
        "    print(\"âŒ File zip dataset tidak ditemukan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "h0m7A67NkXZi",
        "outputId": "24026855-76e9-40e8-df3b-f792e7b49e1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Silakan upload file kaggle.json dari komputer Anda...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b2821dc-d34b-4ed2-9f45-0bb0c6ea5203\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b2821dc-d34b-4ed2-9f45-0bb0c6ea5203\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "ref                                                               title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "----------------------------------------------------------------  --------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "jealousleopard/goodreadsbooks                                     Goodreads-books                                         637338  2020-03-09 09:18:31.583000          81540       1928  1.0              \n",
            "saurabhbagchi/books-dataset                                       Books Dataset                                         25760320  2020-10-09 05:14:41.297000          20415        128  1.0              \n",
            "dylanjcastillo/7k-books-with-metadata                             7k Books                                               1542454  2020-02-04 20:17:23.623000          13508         76  1.0              \n",
            "mohamedbakhet/amazon-books-reviews                                Amazon Books Reviews                                1140283761  2022-09-13 23:04:08.787000          29531        201  1.0              \n",
            "abdallahwagih/books-dataset                                       Books Dataset                                          1657228  2023-12-13 02:26:07.263000           5632         91  1.0              \n",
            "drahulsingh/best-selling-books                                    best-selling-books                                        6413  2023-08-22 17:18:12.903000           8909        166  1.0              \n",
            "elvinrustam/books-dataset                                         Books Dataset                                         55469565  2023-12-20 20:46:34.367000           6690         60  1.0              \n",
            "madankhatri123h/books-dataset                                     Books Dataset                                           366327  2025-06-11 12:38:45.230000           2414         46  1.0              \n",
            "deepcontractor/marvel-comic-books                                 Marvel Comic Books Dataset                             3752218  2022-07-21 11:35:25.140000           4182        185  1.0              \n",
            "asaniczka/amazon-kindle-books-dataset-2023-130k-books             Amazon Kindle Books Dataset 2023 (130K Books)          9555177  2024-03-27 14:56:31.717000           4623        208  1.0              \n",
            "mdhamani/goodreads-books-100k                                     GoodReads 100k books                                  46345587  2021-06-05 12:50:06.680000           3534         56  1.0              \n",
            "sootersaalu/amazon-top-50-bestselling-books-2009-2019             Amazon Top 50 Bestselling Books 2009 - 2019              14857  2020-10-13 09:39:21.180000          77140       1093  1.0              \n",
            "shubhammaindola/harry-potter-books                                Harry Potter Books                                     2390829  2024-05-03 10:35:51.110000           6124         42  1.0              \n",
            "khushikhushikhushi/amazon-bestselling-books                       Amazon Bestselling Books                                  4018  2024-05-23 13:59:52.960000           2474         56  1.0              \n",
            "thedevastator/books-sales-and-ratings                             Books Sales and Ratings                                  54505  2023-12-06 04:54:33.203000           9664         77  1.0              \n",
            "chhavidhankhar11/amazon-books-dataset                             Amazon Books Dataset: Genre, Sub-genre, and Books       558908  2024-02-17 13:46:26.567000           3868         60  1.0              \n",
            "dareenalharthi/jamalon-arabic-books-dataset                       Jamalon Arabic Books Dataset                           1434345  2019-08-15 18:58:06.707000           2432         67  1.0              \n",
            "die9origephit/amazon-data-science-books                           Amazon Data Science Books Dataset                       109367  2023-01-05 03:01:16.353000           6612        111  1.0              \n",
            "agarwalyashhh/best-selling-books                                  Best Selling Books                                        9449  2024-02-08 14:37:00.377000           1988         33  1.0              \n",
            "thomaskonstantin/top-270-rated-computer-science-programing-books  Top 270 Computer Science / Programing Books              46442  2021-01-11 10:13:41.333000           4182         95  1.0              \n",
            "\n",
            "âœ… kaggle.json berhasil diunggah dan terhubung.\n",
            "Dataset URL: https://www.kaggle.com/datasets/mahmoudnoor/high-resolution-catdogbird-image-dataset-13000\n",
            "License(s): copyright-authors\n",
            "Downloading high-resolution-catdogbird-image-dataset-13000.zip to /content/dataset\n",
            " 99% 2.53G/2.56G [00:19<00:00, 200MB/s]\n",
            "100% 2.56G/2.56G [00:19<00:00, 142MB/s]\n",
            "âœ… Dataset berhasil diekstrak di /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "NBEx2nyElSU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Split Dataset"
      ],
      "metadata": {
        "id": "rDWLddnx4-0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Path folder\n",
        "# ===============================\n",
        "dataset_dir = \"/content/dataset\"  # folder asli\n",
        "split_dir = \"/content/split_dataset\"\n",
        "classes = [\"bird\", \"cat\", \"dog\"]\n",
        "\n",
        "train_dir = os.path.join(split_dir, 'train')\n",
        "validation_dir = os.path.join(split_dir, 'validation')\n",
        "test_dir = os.path.join(split_dir, 'test')\n",
        "\n",
        "# ===============================\n",
        "# Hapus split_dataset jika ada\n",
        "# ===============================\n",
        "if os.path.exists(split_dir):\n",
        "    shutil.rmtree(split_dir)\n",
        "\n",
        "# ===============================\n",
        "# Buat folder train, validation, test\n",
        "# ===============================\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(split_dir, split, cls), exist_ok=True)\n",
        "\n",
        "# ===============================\n",
        "# Fungsi ambil semua file gambar\n",
        "# ===============================\n",
        "def list_images(folder):\n",
        "    image_files = []\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\")):\n",
        "                image_files.append(os.path.join(root, file))\n",
        "    return image_files\n",
        "\n",
        "# ===============================\n",
        "# Split data & copy\n",
        "# ===============================\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "for class_name in classes:\n",
        "    class_path = os.path.join(dataset_dir, class_name)\n",
        "    images = list_images(class_path)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    total = len(images)\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end = train_end + int(total * val_ratio)\n",
        "\n",
        "    for i, img_path in enumerate(images):\n",
        "        if i < train_end:\n",
        "            dst = os.path.join(split_dir, \"train\", class_name, os.path.basename(img_path))\n",
        "        elif i < val_end:\n",
        "            dst = os.path.join(split_dir, \"validation\", class_name, os.path.basename(img_path))\n",
        "        else:\n",
        "            dst = os.path.join(split_dir, \"test\", class_name, os.path.basename(img_path))\n",
        "\n",
        "        shutil.copy2(img_path, dst)\n",
        "\n",
        "print(\"âœ… Dataset berhasil dibagi ke folder 'split_dataset'\")\n",
        "\n",
        "# ===============================\n",
        "# Cek jumlah gambar per kelas\n",
        "# ===============================\n",
        "print(\"\\nJumlah gambar per kelas:\")\n",
        "for split in ['train', 'validation', 'test']:\n",
        "    split_path = os.path.join(split_dir, split)\n",
        "    print(f\"\\nğŸ“‚ {split} set:\")\n",
        "    for class_name in os.listdir(split_path):\n",
        "        class_path = os.path.join(split_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            count = len(os.listdir(class_path))\n",
        "            print(f\"  {class_name}: {count} gambar\")\n",
        "\n",
        "# ===============================\n",
        "# ImageDataGenerator\n",
        "# ===============================\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Geb_587M41Mv",
        "outputId": "f08595cf-19d0-46b3-b8cd-5ad33a4449bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset berhasil dibagi ke folder 'split_dataset'\n",
            "\n",
            "Jumlah gambar per kelas:\n",
            "\n",
            "ğŸ“‚ train set:\n",
            "  dog: 3625 gambar\n",
            "  bird: 2904 gambar\n",
            "  cat: 2810 gambar\n",
            "\n",
            "ğŸ“‚ validation set:\n",
            "  dog: 777 gambar\n",
            "  bird: 622 gambar\n",
            "  cat: 602 gambar\n",
            "\n",
            "ğŸ“‚ test set:\n",
            "  dog: 778 gambar\n",
            "  bird: 623 gambar\n",
            "  cat: 603 gambar\n",
            "Found 9339 images belonging to 3 classes.\n",
            "Found 2001 images belonging to 3 classes.\n",
            "Found 2004 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "Kt9Sp3COlVL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 1. Aktifkan mixed precision\n",
        "# ================================\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# ================================\n",
        "# 2. Path dataset\n",
        "# ================================\n",
        "split_dir = \"/content/split_dataset\"\n",
        "train_dir = os.path.join(split_dir, 'train')\n",
        "validation_dir = os.path.join(split_dir, 'validation')\n",
        "test_dir = os.path.join(split_dir, 'test')\n",
        "\n",
        "# ================================\n",
        "# 3. Data Generator\n",
        "# ================================\n",
        "IMG_SIZE = (96, 96)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "num_classes = len(train_generator.class_indices)\n",
        "\n",
        "# ================================\n",
        "# 4. Model MobileNetV2 (Transfer Learning)\n",
        "# ================================\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
        "base_model.trainable = False  # Freeze semua layer dulu\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax', dtype='float32')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callback stop dini saat akurasi â‰¥ 95%\n",
        "class StopOnHighAcc(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') >= 0.95:\n",
        "            print(\"âœ… Akurasi validasi â‰¥ 95%, stop training tahap 1.\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "# ================================\n",
        "# 5. Training Tahap 1 (Cepat)\n",
        "# ================================\n",
        "print(\"\\nğŸš€ Tahap 1: Training cepat (hanya classifier)\")\n",
        "history_stage1 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=20,\n",
        "    callbacks=[StopOnHighAcc()]\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 6. Fine-tuning MobileNetV2\n",
        "# ================================\n",
        "print(\"\\nğŸ”§ Tahap 2: Fine-tuning 15 layer terakhir MobileNetV2\")\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=Adam(learning_rate=1e-5),  # lebih kecil agar stabil\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_stage2 = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10  # cepat\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0PhPYJckbEH",
        "outputId": "9010426b-35c4-4cc3-eb23-fae02eca5693"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9339 images belonging to 3 classes.\n",
            "Found 2001 images belonging to 3 classes.\n",
            "Found 2004 images belonging to 3 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "ğŸš€ Tahap 1: Training cepat (hanya classifier)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 2s/step - accuracy: 0.7816 - loss: 0.5457 - val_accuracy: 0.8906 - val_loss: 0.2755\n",
            "Epoch 2/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 2s/step - accuracy: 0.8892 - loss: 0.2736 - val_accuracy: 0.8891 - val_loss: 0.2811\n",
            "Epoch 3/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 2s/step - accuracy: 0.8836 - loss: 0.2864 - val_accuracy: 0.9050 - val_loss: 0.2424\n",
            "Epoch 4/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m631s\u001b[0m 2s/step - accuracy: 0.9049 - loss: 0.2443 - val_accuracy: 0.9010 - val_loss: 0.2428\n",
            "Epoch 5/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 2s/step - accuracy: 0.9004 - loss: 0.2422 - val_accuracy: 0.9005 - val_loss: 0.2483\n",
            "Epoch 6/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 2s/step - accuracy: 0.9043 - loss: 0.2308 - val_accuracy: 0.8996 - val_loss: 0.2443\n",
            "Epoch 7/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 2s/step - accuracy: 0.9075 - loss: 0.2330 - val_accuracy: 0.9000 - val_loss: 0.2514\n",
            "Epoch 8/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 2s/step - accuracy: 0.9110 - loss: 0.2189 - val_accuracy: 0.9000 - val_loss: 0.2410\n",
            "Epoch 9/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 2s/step - accuracy: 0.9186 - loss: 0.2052 - val_accuracy: 0.8976 - val_loss: 0.2557\n",
            "Epoch 10/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 2s/step - accuracy: 0.9217 - loss: 0.1865 - val_accuracy: 0.9035 - val_loss: 0.2407\n",
            "Epoch 11/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 2s/step - accuracy: 0.9270 - loss: 0.1791 - val_accuracy: 0.8991 - val_loss: 0.2520\n",
            "Epoch 12/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 2s/step - accuracy: 0.9289 - loss: 0.1698 - val_accuracy: 0.8996 - val_loss: 0.2545\n",
            "Epoch 13/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m632s\u001b[0m 2s/step - accuracy: 0.9374 - loss: 0.1559 - val_accuracy: 0.8961 - val_loss: 0.2560\n",
            "Epoch 14/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 2s/step - accuracy: 0.9382 - loss: 0.1585 - val_accuracy: 0.9030 - val_loss: 0.2540\n",
            "Epoch 15/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 2s/step - accuracy: 0.9478 - loss: 0.1389 - val_accuracy: 0.9065 - val_loss: 0.2506\n",
            "Epoch 16/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m634s\u001b[0m 2s/step - accuracy: 0.9431 - loss: 0.1408 - val_accuracy: 0.8996 - val_loss: 0.2524\n",
            "Epoch 17/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m708s\u001b[0m 2s/step - accuracy: 0.9453 - loss: 0.1363 - val_accuracy: 0.8981 - val_loss: 0.2680\n",
            "Epoch 18/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m666s\u001b[0m 2s/step - accuracy: 0.9482 - loss: 0.1265 - val_accuracy: 0.8986 - val_loss: 0.2738\n",
            "Epoch 19/20\n",
            "\u001b[1m292/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 2s/step - accuracy: 0.9530 - loss: 0.1255 - val_accuracy: 0.8951 - val_loss: 0.2778\n",
            "Epoch 20/20\n",
            "\u001b[1m121/292\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5:05\u001b[0m 2s/step - accuracy: 0.9516 - loss: 0.1187"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluasi dan Visualisasi"
      ],
      "metadata": {
        "id": "s9_rlGxLlgq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Konversi Model"
      ],
      "metadata": {
        "id": "yCQGqA_hlkB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# EVALUASI\n",
        "# ===============================\n",
        "loss, accuracy = model.evaluate(test_generator, verbose=2)\n",
        "print(f\"ğŸ“Š Hasil evaluasi:\")\n",
        "print(f\"   Loss     : {loss:.4f}\")\n",
        "print(f\"   Accuracy : {accuracy:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# VISUALISASI\n",
        "# ===============================\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss_hist = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss_hist, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training & Validation Loss')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ===============================\n",
        "# KONVERSI MODEL\n",
        "# ===============================\n",
        "print(\"\\nğŸ’¾ Menyimpan model dalam berbagai format...\")\n",
        "\n",
        "# SavedModel\n",
        "export_dir = 'saved_model/'\n",
        "tf.saved_model.save(model, export_dir)\n",
        "print(\"âœ… SavedModel disimpan di 'saved_model/'\")\n",
        "\n",
        "# TF-Lite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()\n",
        "tflite_model_file = pathlib.Path('tflite/model.tflite')\n",
        "tflite_model_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "print(\"âœ… Model TFLite disimpan di 'tflite/model.tflite'\")\n",
        "\n",
        "# Label.txt\n",
        "with open('tflite/label.txt', 'w') as f:\n",
        "    f.write('\\n'.join(train_generator.class_indices.keys()))\n",
        "print(\"âœ… Label disimpan di 'tflite/label.txt'\")\n",
        "\n",
        "# TensorFlow.js\n",
        "tfjs.converters.save_keras_model(model, 'tfjs_model')\n",
        "print(\"âœ… Model TFJS disimpan di folder 'tfjs_model/'\")"
      ],
      "metadata": {
        "id": "vtpINYV9k2sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unggah gambar uji\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Load label dari file\n",
        "with open('tflite/label.txt', 'r') as f:\n",
        "    class_names = f.read().splitlines()\n",
        "\n",
        "# Load model TFLite\n",
        "interpreter = tf.lite.Interpreter(model_path='tflite/model.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Pastikan target size sama dengan input model\n",
        "target_size = tuple(input_details[0]['shape'][1:3])  # contoh (224, 224)\n",
        "\n",
        "# Preprocessing gambar\n",
        "img = image.load_img(img_path, target_size=target_size)\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "# Inference\n",
        "interpreter.set_tensor(input_details[0]['index'], img_array.astype(np.float32))\n",
        "interpreter.invoke()\n",
        "prediction = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "predicted_index = np.argmax(prediction[0])\n",
        "predicted_class = class_names[predicted_index]\n",
        "confidence = prediction[0][predicted_index]\n",
        "\n",
        "print(f\"Prediksi: {predicted_class} (Probabilitas: {confidence:.2f})\")\n",
        "\n",
        "# Visualisasi hasil\n",
        "plt.imshow(img)\n",
        "plt.title(f'Prediksi: {predicted_class} ({confidence:.2f})')\n",
        "plt.axis('off')\n",
        "plt.savefig('inference_result.png')\n",
        "plt.show()"
      ]
    }
  ]
}
